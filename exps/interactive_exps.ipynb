{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesbian-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/done-sentinel /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/done-sentinel\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/info.json /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/info.json\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/final_layer_norm_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/final_layer_norm_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/input_embeddings_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/input_embeddings_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/output_head_value_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/output_head_value_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/output_unembeddings_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/output_unembeddings_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/position_embedding_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/position_embedding_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0000_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0000_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0001_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0001_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0002_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0002_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0003_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0003_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0004_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0004_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0005_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0005_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0006_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0006_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0007_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0007_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0008_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0008_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0009_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0009_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0010_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0010_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0011_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0011_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0012_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0012_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0013_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0013_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0014_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0014_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0015_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0015_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0016_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0016_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0017_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0017_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0018_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0018_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0019_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0019_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0020_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0020_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0021_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0021_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0022_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0022_shard_000.pkl\n",
      "copying https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0023_shard_000.pkl /tmp/bf-dir-cache/az/openaipublic/summarize-from-feedback/models/sup4_ppo_rm4/checkpoint/resblock_0023_shard_000.pkl\n",
      "Constructing Transformer with the following model hparams:\n",
      "model_H:\n",
      "attn_dropout: 0.0\n",
      "d_model: 2048\n",
      "emb_dropout: 0.0\n",
      "fp16_conv_weights: False\n",
      "fp16_embedding_weights: False\n",
      "heads: 16\n",
      "include_pos_embeddings: True\n",
      "init_scale: 0.7\n",
      "key_bias: None\n",
      "m_attn: 1\n",
      "m_mlp: 4\n",
      "n_ctx: 2048\n",
      "n_layer: 24\n",
      "n_shards: 1\n",
      "res_scale: False\n",
      "resid_dropout: 0.0\n",
      "text_encoding: reversible_50000\n",
      "use_blocksparse_attn: True\n",
      "zero_out: False\n",
      "Loaded model to cuda:0. CUDA memory allocated: 5.68 GB\n",
      "Samples will be written to /tmp/jobs/local/results/samples.0.jsonl\n",
      "================================================================================\n",
      "RESULT 0 of 1\n",
      "CONTEXT:\n",
      "                                                                                                                                                                                                                                                                                                           SUBREDDIT: r/AskReddit\n",
      "\n",
      "TITLE: I have a final for my summer class tomorrow and I thought I'd ask everyone:  What are/were you're pre- college final routines?\n",
      "\n",
      "POST: Some story.  My freshman year of college first semester I took this extremely difficult linear algebra class.  I was between an F-D all semester but arrogant me would just say after every bad test \"Ah I'll figure it out, I don't need to drop.\"  Well it came to the night before the final (a Friday and my first college final) and I still didn't fucking get it.  So I said fuck it and went out and got shit faced with my friends.  I woke up the next morning hung over as all hell and went to take my test.  Low and behold I aced the fucking thing and got a B+ in a class I thought I might fail. I now go drinking the night before every final.  And it keeps working.\n",
      "\n",
      "TL;DR:\n",
      "REF:\n",
      " Was bombing freshman math class.  Got messed up night before final.  Aced it.  Now I do it the night before every final.\n",
      "avg logprob -2.899193548387097\n",
      "SAMPLE 0:\n",
      " Failed my first college Linear Algebra final, went out and get wasted the night before and aced it. Now go drinking whenever I have a final.  What are/were your pre-college final routines?\n",
      "avg logprob -0.7475737658413973\n",
      "Batch 1 of 1.  Took 14.387844324111938 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_path': '/tmp/jobs/local/results'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For testing that environment can run model standalone\n",
    "from summarize_from_feedback import sample\n",
    "from summarize_from_feedback.utils import experiment_helpers as utils\n",
    "from summarize_from_feedback.query_response_model import ModelSpec\n",
    "from summarize_from_feedback.tasks import TaskHParams, TaskQueryHParams, TaskResponseHParams\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = \"local\" #For MPI-dependent logging in sample.main\n",
    "\n",
    "HParams = sample.HParams(\n",
    "     model_spec = ModelSpec(\n",
    "         load_path=\"https://openaipublic.blob.core.windows.net/summarize-from-feedback/models/sup4_ppo_rm4\",\n",
    "         short_name=\"sup4_ppo_rm4\",\n",
    "     ),\n",
    "     task = TaskHParams(\n",
    "         query = TaskQueryHParams(\n",
    "             format_str = \"SUBREDDIT: r/{subreddit}\\n\\nTITLE: {title}\\n\\nPOST: {post}\\n\\nTL;DR:\",\n",
    "             dataset = \"tldr_3_filtered\",\n",
    "             length =  512,\n",
    "             truncate_text = \"\\n\",\n",
    "             truncate_field = \"post\",\n",
    "             pad_side = \"left\"   \n",
    "         ),\n",
    "         response = TaskResponseHParams(\n",
    "             length = 48,\n",
    "             truncate_token = 50256,  # endoftext\n",
    "             ref_format_str = \" {reference}\",  # add a leading space \n",
    "         )\n",
    "     ),\n",
    "     query_dataset_split= \"valid\",\n",
    "     num_queries = 1\n",
    "    )\n",
    "\n",
    "#print(utils.sup4_ppo_rm4())\n",
    "sample.main(H = HParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "combined-glass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Transformer with the following model hparams:\n",
      "model_H:\n",
      "attn_dropout: 0.0\n",
      "d_model: 2048\n",
      "emb_dropout: 0.0\n",
      "fp16_conv_weights: False\n",
      "fp16_embedding_weights: False\n",
      "heads: 16\n",
      "include_pos_embeddings: True\n",
      "init_scale: 0.7\n",
      "key_bias: None\n",
      "m_attn: 1\n",
      "m_mlp: 4\n",
      "n_ctx: 2048\n",
      "n_layer: 24\n",
      "n_shards: 1\n",
      "res_scale: False\n",
      "resid_dropout: 0.0\n",
      "text_encoding: reversible_50000\n",
      "use_blocksparse_attn: True\n",
      "zero_out: False\n",
      "Loaded model to cuda:0. CUDA memory allocated: 11.35 GB\n"
     ]
    }
   ],
   "source": [
    "#For running model on custom input\n",
    "\"\"\"\n",
    "Q's:\n",
    "- How much structure does data need before feeding into policy?(Dict? String?)\n",
    "\"\"\"\n",
    "from summarize_from_feedback import task_data\n",
    "from summarize_from_feedback.policy import Policy\n",
    "\n",
    "H = HParams\n",
    "\n",
    "layout = H.model_spec.run_params.all_gpu_layout()\n",
    "\n",
    "# Instantiate policy\n",
    "policy = Policy(task_hparams=H.task, spec=H.model_spec, layout=layout)\n",
    "\n",
    "#if H.orig_model_spec:\n",
    "#    assert H.orig_model_spec.run_params.n_shards == H.model_spec.run_params.n_shards\n",
    "#    orig_policy = Policy(task_hparams=H.task, spec=H.orig_model_spec, layout=layout)\n",
    "#else:\n",
    "#    orig_policy = None\n",
    "\n",
    "encoder = policy.encoder\n",
    "#response_encoder = ResponseEncoder(H.task.response, encoder)\n",
    "#setup_logging_with_pacific_tz()\n",
    "\n",
    "#act_dtype = torch.float16 if H.fp16_activations else torch.float32\n",
    "\n",
    "#is_logging_rank = layout.is_logging_rank\n",
    "\n",
    "#total_queries_per_replica = exact_div(H.num_queries, layout.n_replicas)\n",
    "#num_runs = exact_div(total_queries_per_replica, H.queries_per_run_per_replica)\n",
    "\n",
    "input_iter = task_data.get_iter_for_task(\n",
    "    H.task,\n",
    "    encoder=encoder,\n",
    "    dataset_split=H.query_dataset_split,\n",
    "    batch_size=H.queries_per_run_per_replica,\n",
    "    layout=layout,\n",
    "    seed=H.seed,\n",
    "    all_fields=True,\n",
    "    )\n",
    "context = next(input_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "round-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#H.task.query.dataset #DEBUG\n",
    "#Padding method for ensuring proper context size for policy\n",
    "PADDING_TOKEN = encoder.encode(\" \")\n",
    "def pad(token_list, desired_shape=512):\n",
    "    diff = len(token_list) - desired_shape\n",
    "    if diff < 0:\n",
    "        token_list = PADDING_TOKEN * abs(diff) + token_list\n",
    "    return token_list[:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "needed-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "raw_context = encoder.decode(context[\"context\"][\"tokens\"])\n",
    "sample = policy.sample(context[\"context\"][\"tokens\"])\n",
    "sample_1 = policy.sample(torch.tensor([pad(encoder.encode(raw_context))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "sized-supplier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I had a difficulty in my lin algebra final a Friday night later and I aced it and now go drinking the night before every final.  What are/were your final pre-college routine?<|endoftext|>\n",
      "\n",
      "In a new book,\n",
      " took intense math class freshman year and didn't get it, woke up the next morning hung over and went to take test, aced final, now go drinking every final night and it works.<|endoftext|>In response to a question on Facebook,\n"
     ]
    }
   ],
   "source": [
    "#Testing out impact of extracted method of encoding + leaving out POST, SUBREDDIT and TL;DR strings in context. \n",
    "for sample in list(map(lambda sample: encoder.decode(sample[\"samples\"]),[sample,sample_1])):\n",
    "     print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor([pad(encoder.encode(raw_context))]).size())\n",
    "print(context[\"context\"][\"tokens\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "beginning-thesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                           SUBREDDIT: r/AskReddit\n",
      "\n",
      "TITLE: I have a final for my summer class tomorrow and I thought I'd ask everyone:  What are/were you're pre- college final routines?\n",
      "\n",
      "POST: Some story.  My freshman year of college first semester I took this extremely difficult linear algebra class.  I was between an F-D all semester but arrogant me would just say after every bad test \"Ah I'll figure it out, I don't need to drop.\"  Well it came to the night before the final (a Friday and my first college final) and I still didn't fucking get it.  So I said fuck it and went out and got shit faced with my friends.  I woke up the next morning hung over as all hell and went to take my test.  Low and behold I aced the fucking thing and got a B+ in a class I thought I might fail. I now go drinking the night before every final.  And it keeps working.\n",
      "\n",
      "TL;DR:\n"
     ]
    }
   ],
   "source": [
    "print(raw_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "backed-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUBREDDIT: r/Math\n",
      "\n",
      "TITLE: What's a theorem that made you furl your eyebrows and say \"what?!\" the first time you saw it?\n",
      "\n",
      "POST: Title is self explanatory. Tell me some theorems/results that are unintuitive or unexpected! Perhaps a good example of this would be seeing the solution to the Bassel Problem for the first time. Bonus points if the theorem/result is either really beautiful or really ugly! Edit: It’s been pointed out to me that most people are unable to fuel their eyebrows, and that the appropriate word here is “furrow”. However, please still feel free to share if a theorem shocked you so much that you indeed managed to furl your eyebrows in disbelief!\n",
      "\n",
      "TL;DR:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_context = \"\"\"\n",
    "SUBREDDIT: r/Math\n",
    "\n",
    "TITLE: What's a theorem that made you furl your eyebrows and say \"what?!\" the first time you saw it?\n",
    "\n",
    "POST: Title is self explanatory. Tell me some theorems/results that are unintuitive or unexpected! Perhaps a good example of this would be seeing the solution to the Bassel Problem for the first time. Bonus points if the theorem/result is either really beautiful or really ugly! Edit: It’s been pointed out to me that most people are unable to fuel their eyebrows, and that the appropriate word here is “furrow”. However, please still feel free to share if a theorem shocked you so much that you indeed managed to furl your eyebrows in disbelief!\n",
    "\n",
    "TL;DR:\n",
    "\"\"\"\n",
    "print(custom_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "affiliated-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-346\n",
      "Output:  concluding some math problems for the first time, saw a theorem that made you furrow your eyebrows and say \"what?!\" the first time you saw it. Share your own experiences with theorems and results!<|endoftext|>This is a post\n"
     ]
    }
   ],
   "source": [
    "custom_sample = policy.sample(torch.tensor([pad(encoder.encode(custom_context))]))\n",
    "print(\"Output: {}\".format(encoder.decode(custom_sample[\"samples\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-breeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (custom_python)",
   "language": "python",
   "name": "custom_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
